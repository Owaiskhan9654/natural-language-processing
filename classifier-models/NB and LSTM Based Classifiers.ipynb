{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Ruthu S Sanketh\n",
    "\n",
    "### NB and LSTM based classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this tutorial is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ElRkQElWUMjG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, keras, string, re, html, math\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fhHRim2AUm4z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape -  (50000, 2) \n",
      "\n",
      "The number of null values -  review 0\n",
      "The number of null values -  sentiment 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the IMDB dataset. We load it using pandas as dataframe\n",
    "data = pd.read_csv('/Users/ruthu/Desktop/IMDB Dataset.csv') \n",
    "print(\"Data shape - \", data.shape, \"\\n\")                                  #prints the number of rows and columns\n",
    "\n",
    "for col in data.columns:\n",
    "    print(\"The number of null values - \", col, data[col].isnull().sum())   #prints the number of null values in each column\n",
    "\n",
    "data[\"review\"]= data[\"review\"].str.lower() \n",
    "data[\"sentiment\"]= data[\"sentiment\"].str.lower()             #converts every value in the column to lowercase\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_Hn2f6VMP7"
   },
   "source": [
    "# Preprocessing\n",
    "Pre-precessing that needs to be done on lower cased corpus - \n",
    "\n",
    "1. Removal of html tags\n",
    "2. Removal of  URLS\n",
    "3. Removal of non alphanumeric character\n",
    "4. Removal of Stopwords\n",
    "5. Performing stemming and lemmatization\n",
    "\n",
    "We use regex from re. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "      <td>[one, reviewer, mentioned, watching, 1, oz, ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>[basically, family, little, boy, jake, think, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>[petter, mattei, love, time, money, visually, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  one of the other reviewers has mentioned that ...  positive   \n",
       "1  a wonderful little production. <br /><br />the...  positive   \n",
       "2  i thought this was a wonderful way to spend ti...  positive   \n",
       "3  basically there's a family where a little boy ...  negative   \n",
       "4  petter mattei's \"love in the time of money\" is...  positive   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  one reviewer mentioned watching 1 oz episode h...   \n",
       "1  wonderful little production filming technique ...   \n",
       "2  thought wonderful way spend time hot summer we...   \n",
       "3  basically family little boy jake think zombie ...   \n",
       "4  petter mattei love time money visually stunnin...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [one, reviewer, mentioned, watching, 1, oz, ep...  \n",
       "1  [wonderful, little, production, filming, techn...  \n",
       "2  [thought, wonderful, way, spend, time, hot, su...  \n",
       "3  [basically, family, little, boy, jake, think, ...  \n",
       "4  [petter, mattei, love, time, money, visually, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning(data):\n",
    "    clean = re.sub('<.*?>', ' ', str(data))            #removes HTML tags\n",
    "    clean = re.sub('\\'.*?\\s',' ', clean)               #removes all hanging letters afer apostrophes (s in it's)\n",
    "    clean = re.sub(r'http\\S+',' ', clean)              #removes URLs\n",
    "    clean = re.sub('\\W+',' ', clean)                   #replacing the non alphanumeric characters\n",
    "    return html.unescape(clean)\n",
    "data['cleaned'] = data['review'].apply(cleaning)\n",
    "\n",
    "\n",
    "def tokenizing(data):\n",
    "    review = data['cleaned']                            #tokenizing is done\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    return tokens\n",
    "data['tokens'] = data.apply(tokenizing, axis=1)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stops(data):\n",
    "    my_list = data['tokens']\n",
    "    meaningful_words = [w for w in my_list if not w in stop_words]           #stopwords are removed from the tokenized data\n",
    "    return (meaningful_words)\n",
    "data['tokens'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(data):\n",
    "    my_list = data['tokens']\n",
    "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_list]    #lemmatizing is performed. It's more efficient and better than stemming.\n",
    "    return (lemmatized_list)\n",
    "data['tokens'] = data.apply(lemmatizing, axis=1)\n",
    "\n",
    "def rejoin_words(data):\n",
    "    my_list = data['tokens']\n",
    "    joined_words = ( \" \".join(my_list))                     #rejoins all stemmed words\n",
    "    return joined_words\n",
    "data['cleaned'] = data.apply(rejoin_words, axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DyaSkfcvYGXk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is -  544935\n",
      "\n",
      "The number of tokens is -  5961690\n",
      "\n",
      "The average number of tokens per sentence is -  11\n",
      "\n",
      "The number of positive examples are -  25000\n",
      "\n",
      "The number of negative examples are -  25000\n",
      "\n",
      "The proportion of positive sentiments to negative ones are -  1.0\n"
     ]
    }
   ],
   "source": [
    "# Prints statistics of Data like avg length of sentence , proportion of data w.r.t class labels\n",
    "def sents(data):\n",
    "    clean = re.sub('<.*?>', ' ', str(data))            #removes HTML tags\n",
    "    clean = re.sub('\\'.*?\\s',' ', clean)               #removes all hanging letters afer apostrophes (s in it's)\n",
    "    clean = re.sub(r'http\\S+',' ', clean)              #removes URLs\n",
    "    clean = re.sub('[^a-zA-Z0-9\\.]+',' ', clean)       #removes all non-alphanumeric characters except periods.\n",
    "    tokens = nltk.sent_tokenize(clean)                 #sentence tokenizing is done\n",
    "    return tokens\n",
    "sents = data['review'].apply(sents)\n",
    "\n",
    "length_s = 0\n",
    "for i in range(data.shape[0]):\n",
    "    length_s+= len(sents[i])\n",
    "print(\"The number of sentences is - \", length_s)          #prints the number of sentences\n",
    "\n",
    "length_t = 0\n",
    "for i in range(data.shape[0]):\n",
    "    length_t+= len(data['tokens'][i])\n",
    "print(\"\\nThe number of tokens is - \", length_t)           #prints the number of tokens\n",
    "\n",
    "average_tokens = round(length_t/length_s)\n",
    "print(\"\\nThe average number of tokens per sentence is - \", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "positive = negative = 0\n",
    "for i in range(data.shape[0]):\n",
    "    if (data['sentiment'][i]=='positive'):\n",
    "        positive += 1                           #finds the proprtion of positive and negative sentiments\n",
    "    else:\n",
    "        negative += 1\n",
    "\n",
    "print(\"\\nThe number of positive examples are - \", positive)\n",
    "print(\"\\nThe number of negative examples are - \", negative)\n",
    "print(\"\\nThe proportion of positive sentiments to negative ones are - \", positive/negative)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkJ-e2pUwun"
   },
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eVq-mN28U_J4"
   },
   "outputs": [],
   "source": [
    "# gets reviews column from df\n",
    "reviews = data['cleaned'].values\n",
    "\n",
    "# gets labels column from df\n",
    "labels = data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ljo5NquhXTXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: encoded, dtype: int32\n",
      "\n",
      "The encoded classes are -  {'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "# Uses label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "data['encoded']= encoded_labels\n",
    "print(data['encoded'].head())\n",
    "\n",
    "# prints(enc.classes_)\n",
    "encoder_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"\\nThe encoded classes are - \", encoder_mapping)\n",
    "\n",
    "labels = data['encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wzG-C_EVWWET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training sentences are -\n",
      "\n",
      "['caught little gem totally accident back 1980 revival theatre see two old silly sci fi movie theatre packed full warning showed bunch sci fi short spoof get u mood somewhat amusing came within second audience hysteric biggest laugh came showed princess laia huge cinnamon bun instead hair head look camera give grim smile nod made even funnier got ta see chewabacca played look like muppet extremely silly stupid stop laughing dialogue drowned laughter also know star war pretty well even funnier deliberately poke fun dialogue really work audience definite 10'\n",
      " 'believe let movie accomplish favor friend ask early april 14 2007 movie certainly pain as theater sickly boring even felt gory impact daunting scene deem complete failure attract audience worst even trampled cause friend failed come time theater busy assisting boyfriend looking appropriate lodge stay one night really disappointed matter movie matter indeed poor plot useless storyline naively created know say anymore title suggest anyway creep horror failed overture u viewer maybe beating animal could get creep show theater real situational play good luck anyone attempt watch anyway'\n",
      " 'spoiler alert get nerve people remake use term loosely good movie american version dutch thriller someone decided original ending pasteurized enough american audience create new one stupid improbable pretend kind ending favor get original one'\n",
      " ...\n",
      " 'waste time danger watch tempted tear dvd wall heave thru window amateur production terrible repetitive vacuous dialog paper thin plot line wooden performance lucy lawless pathetically hackneyed seriously flawed story completely unbelievable character two worst concept film v 1 evil twin 2 amnesia twin plot twist outrageously simplistic obvious like watching train coming track middle day prairie even resolve properly evil punished original crime please please please watch even free choice go synagogue'\n",
      " 'far pathetic movie indian cinema cinema come totally piece crap story line ode hold water shameful respectful actor stooped low making disgraceful movie little respect lost forever retired longtime ago instead making fool loosing self respect would recommend movie one furthermore would suggest amitabh retire already wonder imdb given recommendation movie movie horrible recommendation could made seeing movie devdas line movie grave injustice decent movie overall could say shameless'\n",
      " 'movie forever left impression watched freshman high school home alone night think lost respect robert reed actor huge fan brady bunch also thought role chuck connor horrendous evil however movie made impact volunteer woman state prison bible study church service trying change woman life one time fascinates people actually watched movie none friend watched family clueless day discus movie see']\n",
      "\n",
      "The test sentences are -\n",
      "\n",
      "['yes mtv really way market daria started clever teenage angst comment everything suck make viewer feel better sucky teenage life sitcom mutated deal problem charade used watch daria time loved sitting watching called movie wonder point daria tell u lead life college excuse point daria made every episode like ok long ok matter rest sick sad world think entire thing reminded scene reality bite movie channel show documentry first time'\n",
      " 'story bride fair amusing engaging one filmmaker credit set portray rural minnesotan respect ordinarily reserved coast dweller weird though find independent movie brainchild single person unambitious cliché ridden committee brewed hollywood potboiler portrait rural people intended affectionate think character ring true quite meal small town diner never overheard debate merit different nineteenth century english novelist one might suggest writer director semans experience rural culture coen brother considerably le satiric verve'\n",
      " 'team varied scully mulder two scientist pilot guy play bana seinfeld go arctic research post member died either killing killing discover worm virus parasitic point madness death problem certain dog lash anyone could infected favorite episode season 1 also one favorite show arctic environment encloses character course like carpenter thing lot fun watching even tempered character suddenly start flip dramatic scene visual effect worm effect skin cheesy mind drama character end working would usually tension actor including bana guy understand going story usual loved ambiguity ending highly recommended'\n",
      " ...\n",
      " 'film seen made uwe boll knew probably worst director ever always make film based video game also house dead one imdb bottom 100 still wanted watch huge fan game wanted see doe film make bad watching agree crap movie story first 15 20 minute nothing topless teenage girl brain running moment wondering zombie brain dead girl night time zombie popped nowhere started attacking people later woman started shooting mean take one place every 5 minute supposed comedy horror knew fell asleep second half woke end credit manage watch good thing film true insult classic game uwe boll please make film thank'\n",
      " 'story shin ae move milyang seoul young son jun start accidental death husband husband born opening piano school also ambition land insurance money received death film probably would like hollywood film falling local guy happy son new home hollywood son get kidnapped murdered ostensibly known cash settlement grief process attempt moving attempt clear conscience guilt done admirably lead actress superb caveat stated depressing film know going want shin ae go grief find measure happiness hollywood korea korean cinema especially drama pull punch life happens great acting sometimes tough film watch due going stay rewarded'\n",
      " 'leon errol handle double role uncle matt lindsay lord basil epping superbly trouble liking mexican spitfire series contrived produce mistaken identity telegraphed way advance errol funny stuffy lord epping would preferred lot wit much le repetition']\n",
      "\n",
      "The training labels are -\n",
      "\n",
      "47808    1\n",
      "20154    0\n",
      "43069    0\n",
      "19413    0\n",
      "13673    0\n",
      "        ..\n",
      "31092    1\n",
      "22917    0\n",
      "47481    0\n",
      "35597    0\n",
      "27491    0\n",
      "Name: encoded, Length: 40000, dtype: int32\n",
      "\n",
      "The test labels are -\n",
      "\n",
      "18870    0\n",
      "39791    0\n",
      "30381    1\n",
      "42294    0\n",
      "33480    0\n",
      "        ..\n",
      "3634     0\n",
      "47910    0\n",
      "16086    0\n",
      "48294    1\n",
      "4478     0\n",
      "Name: encoded, Length: 10000, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Splits the data into train and test (80% - 20%). \n",
    "# Uses stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# train_sentences, test_sentences, train_labels, test_labels\n",
    "print(\"The training sentences are -\",train_sentences, sep='\\n\\n')\n",
    "print(\"\\nThe test sentences are -\",test_sentences, sep='\\n\\n')\n",
    "print(\"\\nThe training labels are -\",train_labels, sep='\\n\\n')\n",
    "print(\"\\nThe test labels are -\",test_labels, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz1YdsSkiWCX"
   },
   "source": [
    "There are two approaches possible for building vocabulary for the Naive Bayes classifier.\n",
    "1. We take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
    "2. We take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one of the probability terms are not zero.\n",
    " \n",
    "We use the 2nd approach.\n",
    " \n",
    "Also, building vocab by taking all words in the train set is memory intensive, hence we build the vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
    "\n",
    "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
    "\n",
    "\n",
    "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
    "\n",
    "$N_{w_j}$ : Total count of features in class $w_j$\n",
    "\n",
    "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
    "\n",
    "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1cllNfGmUr77"
   },
   "outputs": [],
   "source": [
    "# Uses Count vectorizer to get frequency of the words\n",
    "vectorizer = CountVectorizer(max_features = 3000)\n",
    "\n",
    "sents_encoded = vectorizer.fit_transform(train_sentences)         #encodes all training sentences\n",
    "counts = sents_encoded.sum(axis=0).A1\n",
    "vocab = list(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iE7pxWIYW1z0"
   },
   "outputs": [],
   "source": [
    "# Builds the model.\n",
    "# Uses laplace smoothing for words in test set not present in vocab of train set\n",
    "class MultinomialNaiveBayes:\n",
    "  \n",
    "    def __init__(self, classes, tokenizer):\n",
    "      #self.tokenizer = tokenizer\n",
    "      self.classes = classes\n",
    "      \n",
    "    def group_by_class(self, X, y):\n",
    "      data = dict()\n",
    "      for c in self.classes:                            #grouping by positive and negative sentiments\n",
    "        data[c] = X[np.where(y == c)]\n",
    "      return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = vocab                            #using the pre-made vocabulary of 3000 most frequent training words\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "          self.n_class_items[c] = len(data)\n",
    "          self.log_class_priors[c] = math.log(self.n_class_items[c] / n)   #taking log for easier calculation\n",
    "          self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "          for text in data:\n",
    "            counts = Counter(nltk.word_tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                self.word_counts[c][word] += count\n",
    "                \n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class):          #smoothing\n",
    "      num = self.word_counts[text_class][word] + 1\n",
    "      denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "      return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:\n",
    "          \n",
    "          class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "          words = set(nltk.word_tokenize(text))\n",
    "          for word in words:\n",
    "              if word not in self.vocab: continue\n",
    "\n",
    "              for c in self.classes:\n",
    "                \n",
    "                log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                class_scores[c] += log_w_given_c\n",
    "                \n",
    "          result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AtQSl1zvW4DD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the MNB classifier is  0.8533\n",
      "\n",
      "The classification report with metrics - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      5000\n",
      "           1       0.85      0.86      0.85      5000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNaiveBayes(\n",
    "    classes=np.unique(labels), \n",
    "    tokenizer=Tokenizer()\n",
    ").fit(train_sentences, train_labels)\n",
    "\n",
    "# Tests the model on test set and reports the Accuracy\n",
    "predicted_labels = MNB.predict(test_sentences)\n",
    "print(\"The accuracy of the MNB classifier is \", accuracy_score(test_labels, predicted_labels))\n",
    "print(\"\\nThe classification report with metrics - \\n\", classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNql0acU7sa"
   },
   "source": [
    "# LSTM based Classifier\n",
    "\n",
    "We use the above train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SkqnvbUOXoN0"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 150\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "# tokenizes sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# converts train dataset to sequence and pads sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# converts Test dataset to sequence and pads sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Mtw3w895ZP39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 100)          8295000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 8,382,601\n",
      "Trainable params: 8,382,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compiles model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "skmaDJMnZTzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 158s 140ms/step - loss: 0.3453 - accuracy: 0.8511 - val_loss: 0.2583 - val_accuracy: 0.8967\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 181s 160ms/step - loss: 0.1598 - accuracy: 0.9421 - val_loss: 0.2930 - val_accuracy: 0.8870\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 181s 161ms/step - loss: 0.0805 - accuracy: 0.9735 - val_loss: 0.4266 - val_accuracy: 0.8815\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 182s 162ms/step - loss: 0.0490 - accuracy: 0.9835 - val_loss: 0.4797 - val_accuracy: 0.8733\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 185s 165ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.4653 - val_accuracy: 0.8655\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TjEhWEr5Zq7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities are - \n",
      "[[9.8351729e-01]\n",
      " [3.4398127e-01]\n",
      " [9.9961859e-01]\n",
      " ...\n",
      " [2.4896860e-04]\n",
      " [9.9665564e-01]\n",
      " [3.7850103e-01]]\n",
      "\n",
      "The labels are - \n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "The accuracy of the model is  0.8711\n",
      "\n",
      "The accuracy and other metrics are \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      5000\n",
      "           1       0.89      0.84      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gets probabilities\n",
    "prediction = model.predict(test_padded)\n",
    "print(\"The probabilities are - \", prediction, sep='\\n')\n",
    "\n",
    "# Gets labels based on probability 1 if p>= 0.5 else 0\n",
    "for each in prediction:\n",
    "    if each[0] >=0.5:\n",
    "        each[0] = 1\n",
    "    else:\n",
    "        each[0] = 0\n",
    "prediction = prediction.astype('int32') \n",
    "print(\"\\nThe labels are - \", prediction, sep='\\n')\n",
    "\n",
    "# Calculates accuracy on Test data\n",
    "print(\"\\nThe accuracy of the model is \", accuracy_score(test_labels, prediction))\n",
    "print(\"\\nThe accuracy and other metrics are \\n\", classification_report(test_labels, prediction, labels=[0, 1]),sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIICV-ySOYL0"
   },
   "source": [
    "## To get predictions for random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m2RmfNL3OYL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities are - \n",
      "[[0.96641695]\n",
      " [0.03413102]\n",
      " [0.0519332 ]]\n",
      "\n",
      "The labels are - \n",
      "[[1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the movie plot is terrible but it had good acting\"]\n",
    "\n",
    "# converts to a sequence\n",
    "test_sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "# pads the sequence\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Gets probabilities\n",
    "prediction = model.predict(test_padded)\n",
    "print(\"The probabilities are - \", prediction, sep='\\n')\n",
    "\n",
    "# Gets labels based on probability 1 if p>= 0.5 else 0\n",
    "for each in prediction:\n",
    "    if each[0] >=0.5:\n",
    "        each[0] = 1\n",
    "    else:\n",
    "        each[0] = 0\n",
    "prediction = prediction.astype('int32') \n",
    "print(\"\\nThe labels are - \", prediction, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the MNB classifier has an accuracy of around 85%, while the LSTM classifier has an accuracy of around 87%, and is hence the better classifier."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment -3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
