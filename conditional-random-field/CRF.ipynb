{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Ruthu S Sanketh\n",
    "\n",
    "The objective of this tutorial is to experiment with POS tagging which is a standard sequence labeling task using Conditional Random Field (CRF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the needed libraries\n",
    "import pandas as pd       \n",
    "import nltk\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "import math, string, re\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID    WORD POS_TAG\n",
      "0      1.0    yaha     DET\n",
      "1      2.0   eSiyA   PROPN\n",
      "2      3.0      kI     ADP\n",
      "3      4.0  sabase     ADV\n",
      "4      5.0   badZI     ADJ\n",
      "...    ...     ...     ...\n",
      "8105   9.0   TaMdI     ADJ\n",
      "8106  10.0      ho    VERB\n",
      "8107  11.0    jAwI     AUX\n",
      "8108  12.0      hE     AUX\n",
      "8109  13.0       .   PUNCT\n",
      "\n",
      "[8110 rows x 3 columns]\n",
      "\n",
      "        ID      WORD    TAG\n",
      "0      1.0  rAmAyaNa  PROPN\n",
      "1      2.0      kAla  PROPN\n",
      "2      3.0       meM    ADP\n",
      "3      4.0  BagavAna   NOUN\n",
      "4      5.0      rAma  PROPN\n",
      "...    ...       ...    ...\n",
      "1552  10.0     ISAna  PROPN\n",
      "1553  11.0        kA    ADP\n",
      "1554  12.0   maMxira   NOUN\n",
      "1555  13.0        hE    AUX\n",
      "1556  14.0         .  PUNCT\n",
      "\n",
      "[1557 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#reading and storing the data\n",
    "data = {}\n",
    "data['train'] = pd.read_csv('/Users/ruthu/Desktop/hi-ud-train.conllu')\n",
    "data['test'] = pd.read_csv('/Users/ruthu/Desktop/hi-ud-test.conllu', sep = '\\t')\n",
    "\n",
    "print(data['train'], data['test'], sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Used - <br>\n",
    "1.   The word\n",
    "2.   The word in lowercase\n",
    "3.   Prefixes and suffixes of the word of varying lengths\n",
    "4.   If the word is a digit\n",
    "5.   If the word is a punctuation mark\n",
    "6.   If the word is at the beginning of the sentence (BOS) or the end of the sentence (EOS) or neither\n",
    "7.   The length of the word- no. of characters (since shorter words are expected to be more likely to belong to a particular POS, eg. prepositions, pronouns)\n",
    "8.   Stemmed version of the word, which deletes all vowels along with g, y, n from the end of the word, but leaves at least a 2 character long stem\n",
    "9.   Features mentioned above for the previous word, the following word, and the words two places before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract features\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'len(word)': len(word),\n",
    "        'word[:4]': word[:4],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-4:]': word[-4:],\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word.lower()),\n",
    "        'word.ispunctuation': (word in string.punctuation),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word1.lower()),\n",
    "            '-1:word[:3]': word1[:3],\n",
    "            '-1:word[:2]': word1[:2],\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.ispunctuation': (word1 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        features.update({\n",
    "            '-2:word': word2,\n",
    "            '-2:len(word)': len(word2),\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word[:3]': word2[:3],\n",
    "            '-2:word[:2]': word2[:2],\n",
    "            '-2:word[-3:]': word2[-3:],\n",
    "            '-2:word[-2:]': word2[-2:],\n",
    "            '-2:word.isdigit()': word2.isdigit(),\n",
    "            '-2:word.ispunctuation': (word2 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word': word1,\n",
    "            '+1:len(word)': len(word1),\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word[:3]': word1[:3],\n",
    "            '+1:word[:2]': word1[:2],\n",
    "            '+1:word[-3:]': word1[-3:],\n",
    "            '+1:word[-2:]': word1[-2:],\n",
    "            '+1:word.isdigit()': word1.isdigit(),\n",
    "            '+1:word.ispunctuation': (word1 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    if i < len(sent) - 2:\n",
    "        word2 = sent[i+2][0]\n",
    "        features.update({\n",
    "            '+2:word': word2,\n",
    "            '+2:len(word)': len(word2),\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word2.lower()),\n",
    "            '+2:word[:3]': word2[:3],\n",
    "            '+2:word[:2]': word2[:2],\n",
    "            '+2:word[-3:]': word2[-3:],\n",
    "            '+2:word[-2:]': word2[-2:],\n",
    "            '+2:word.isdigit()': word2.isdigit(),\n",
    "            '+2:word.ispunctuation': (word2 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [word[1] for word in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word[0] for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data into sentences\n",
    "def format_data(csv_data):\n",
    "    sents = []\n",
    "    for i in range(len(csv_data)):\n",
    "        if math.isnan(csv_data.iloc[i, 0]):\n",
    "            continue\n",
    "        elif csv_data.iloc[i, 0] == 1.0:\n",
    "            sents.append([[csv_data.iloc[i, 1], csv_data.iloc[i, 2]]])\n",
    "        else:\n",
    "            sents[-1].append([csv_data.iloc[i, 1], csv_data.iloc[i, 2]])\n",
    "    for sent in sents:\n",
    "        for i, word in enumerate(sent):\n",
    "            if type(word[0]) != str:\n",
    "                del sent[i]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting features from all the sentences\n",
    "train_sents = format_data(data['train'])\n",
    "test_sents = format_data(data['test'])\n",
    "\n",
    "Xtrain = [sent2features(s) for s in train_sents]\n",
    "ytrain = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "Xtest = [sent2features(s) for s in test_sents]\n",
    "ytest = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.25, c2=0.3,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time                                  \n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    c1 = 0.25,\n",
    "    c2 = 0.3,\n",
    "    max_iterations = 100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(Xtrain, ytrain)                  #training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the train set = 0.9989320264718571\n",
      "\n",
      "Accuracy on the train set = 0.9989329064959317\n",
      "\n",
      "Train set classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PART      1.000     1.000     1.000       163\n",
      "       CCONJ      1.000     1.000     1.000       150\n",
      "       SCONJ      1.000     1.000     1.000        61\n",
      "         ADJ      1.000     1.000     1.000       570\n",
      "         ADP      1.000     1.000     1.000      1387\n",
      "         ADV      1.000     0.991     0.995       111\n",
      "        VERB      1.000     0.991     0.995       640\n",
      "         DET      1.000     0.996     0.998       231\n",
      "        NOUN      0.999     1.000     1.000      1597\n",
      "        PRON      0.998     1.000     0.999       431\n",
      "       PROPN      1.000     1.000     1.000       708\n",
      "         NUM      1.000     1.000     1.000       152\n",
      "       PUNCT      1.000     1.000     1.000       564\n",
      "         AUX      0.992     1.000     0.996       730\n",
      "\n",
      "   micro avg      0.999     0.999     0.999      7495\n",
      "   macro avg      0.999     0.998     0.999      7495\n",
      "weighted avg      0.999     0.999     0.999      7495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#obtaining metrics such as accuracy, etc. on the train set\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('X')\n",
    "\n",
    "ypred = crf.predict(Xtrain)\n",
    "print('F1 score on the train set = {}\\n'.format(metrics.flat_f1_score(ytrain, ypred, average='weighted', labels=labels)))\n",
    "print('Accuracy on the train set = {}\\n'.format(metrics.flat_accuracy_score(ytrain, ypred)))\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print('Train set classification report: \\n\\n{}'.format(metrics.flat_classification_report(\n",
    "    ytrain, ypred, labels=sorted_labels, digits=3\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test set = 0.8674205655336537\n",
      "\n",
      "Accuracy on the test set = 0.8683127572016461\n",
      "\n",
      "Test set classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PART      1.000     0.879     0.935        33\n",
      "       CCONJ      1.000     1.000     1.000        25\n",
      "       SCONJ      0.667     0.667     0.667         3\n",
      "         ADJ      0.676     0.777     0.723        94\n",
      "         ADP      0.967     0.955     0.961       309\n",
      "         ADV      0.583     0.333     0.424        21\n",
      "        VERB      0.935     0.869     0.901        99\n",
      "         DET      0.795     0.861     0.827        36\n",
      "        NOUN      0.785     0.863     0.822       329\n",
      "        PRON      0.929     0.800     0.860        65\n",
      "       PROPN      0.692     0.621     0.655       145\n",
      "         NUM      1.000     0.880     0.936        25\n",
      "       PUNCT      1.000     0.993     0.996       135\n",
      "         AUX      0.965     0.978     0.971       139\n",
      "\n",
      "    accuracy                          0.868      1458\n",
      "   macro avg      0.857     0.820     0.834      1458\n",
      "weighted avg      0.870     0.868     0.867      1458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#obtaining metrics such as accuracy, etc. on the test set\n",
    "ypred = crf.predict(Xtest)\n",
    "print('F1 score on the test set = {}\\n'.format(metrics.flat_f1_score(ytest, ypred,\n",
    "                      average='weighted', labels=labels)))\n",
    "print('Accuracy on the test set = {}\\n'.format(metrics.flat_accuracy_score(ytest, ypred)))\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print('Test set classification report: \\n\\n{}'.format(metrics.flat_classification_report(\n",
    "    ytest, ypred, labels=sorted_labels, digits=3\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 likely transitions - \n",
      "\n",
      "VERB   -> AUX     3.496273\n",
      "PROPN  -> PROPN   1.951488\n",
      "AUX    -> AUX     1.816472\n",
      "ADJ    -> NOUN    1.766149\n",
      "AUX    -> SCONJ   1.578886\n",
      "NUM    -> NOUN    1.525581\n",
      "PART   -> NUM     1.421199\n",
      "DET    -> NOUN    1.412161\n",
      "VERB   -> SCONJ   1.334200\n",
      "PRON   -> ADP     1.142136\n",
      "\n",
      "Top 10 unlikely transitions - \n",
      "\n",
      "VERB   -> ADJ     -0.897578\n",
      "DET    -> PROPN   -0.964973\n",
      "PROPN  -> NOUN    -1.016041\n",
      "PROPN  -> PART    -1.050191\n",
      "PROPN  -> AUX     -1.053615\n",
      "DET    -> ADP     -1.121233\n",
      "PROPN  -> DET     -1.279584\n",
      "ADJ    -> PRON    -1.281203\n",
      "VERB   -> VERB    -1.401497\n",
      "ADJ    -> ADP     -2.176380\n"
     ]
    }
   ],
   "source": [
    "#obtaining the most likely and the least likely transitions \n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(transition_features):\n",
    "    for (label_from, label_to), weight in transition_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top 10 likely transitions - \\n\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 unlikely transitions - \\n\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
